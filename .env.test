# LLM Provider Selection
# Options: ollama, openai, anthropic, replicate, deepseek, bedrock, custom
DEFAULT_LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=deepseek-r1:1.5b

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key
OPENAI_DEFAULT_MODEL=gpt-4o
OPENAI_API_BASE=https://api.openai.com/v1

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key
ANTHROPIC_DEFAULT_MODEL=claude-3-opus-20240229
ANTHROPIC_API_BASE=https://api.anthropic.com/v1

# Replicate Configuration
REPLICATE_API_KEY=your_replicate_api_key
REPLICATE_DEFAULT_MODEL=meta/llama-3-70b-instruct:2a30680ab9b12307db19bce8c7e74da618c76c9eba31db02e4b3088d9e9ee5b2
REPLICATE_API_BASE=https://api.replicate.com/v1

# Deepseek Configuration (if using API directly)
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_DEFAULT_MODEL=deepseek-chat
DEEPSEEK_API_BASE=https://api.deepseek.com/v1

# Amazon Bedrock Configuration
AWS_ACCESS_KEY_ID=your_aws_access_key_id
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key
AWS_REGION=us-east-1
BEDROCK_DEFAULT_MODEL=anthropic.claude-3-sonnet-20240229-v1:0

# Custom LLM Configuration
CUSTOM_LLM_API_BASE=https://your-custom-llm-endpoint.com/api
CUSTOM_LLM_API_KEY=your_custom_api_key
CUSTOM_LLM_DEFAULT_MODEL=your-custom-model

# Default LLM Settings (used across providers)
TEMPERATURE=0.7
MAX_TOKENS=2048
REQUEST_TIMEOUT=60.0
DEFAULT_PROMPT_TYPE=general
